{"paragraphs":[{"text":"%md\n# LAB08: Machine Learning with SPARK","user":"yava","dateUpdated":"2019-12-20T07:11:00-0500","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>LAB08: Machine Learning with SPARK</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1576843772940_-1785093876","id":"20191220-070932_830578484","dateCreated":"2019-12-20T07:09:32-0500","dateStarted":"2019-12-20T07:11:00-0500","dateFinished":"2019-12-20T07:11:00-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:103369"},{"text":"%spark.pyspark\nprint(\"Spark version : \" + spark.version)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687394_-620973523","id":"20191218-160248_1047624034","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103370"},{"text":"%spark.pyspark\nimport sys\nprint(\"Python version : \" + sys.version)\n","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687395_-288801988","id":"20191218-161024_1354661735","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103371"},{"text":"%spark.pyspark\n#import re\n#import numpy\nimport pyspark.sql.functions as F\n#import pandas\n#import matplotlib\n#import seaborn","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687395_-26025136","id":"20191218-161229_999178797","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103372"},{"text":"","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687396_-649189751","id":"20191218-161642_1434812174","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103373"},{"text":"%spark.pyspark\n\nfile_path = \"hdfs://<ip-addr>/user/yava/dataset/stg_youtube/data/000000_0\"\nyoutube_schema=StructType([\n    StructField(\"video_id\", StringType(), True),\n    StructField(\"trending_date\", IntegerType(), True),\n    StructField(\"title\", StringType(), True),\n    StructField(\"channel_title\", StringType(), True),\n    StructField(\"category_id\", IntegerType(), True),\n    StructField(\"publish_time\", StringType(), True),\n    StructField(\"tags\", StringType(), True),\n    StructField(\"views\", IntegerType(), True),\n    StructField(\"likes\", IntegerType(), True),\n    StructField(\"dislikes\", IntegerType(), True),\n    StructField(\"comment_count\", IntegerType(), True)\n])\n\n\ndf_youtube = spark.read \\\n .option(\"delimiter\", \"\\t\") \\\n .schema(youtube_schema) \\\n .option(\"inferSchema\", \"True\") \\\n .csv(file_path)","user":"yava","dateUpdated":"2019-12-20T07:10:32-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687396_-749754130","id":"20191218-161946_1376047672","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103374"},{"text":"%spark.pyspark\ndf_youtube","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687396_-1046689899","id":"20191218-162640_916302512","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103375"},{"text":"%spark.pyspark\ndf_youtube.count()","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687397_-1993448327","id":"20191218-174702_1169312433","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103376"},{"text":"%spark.pyspark\n\n# Explore data :\n# 1. show the first 5 rows\n# 2. print schema\n\ndf_youtube.show(5,True)\ndf_youtube.printSchema()\n\n\n#df_test = df_youtube.select(['category_id', 'likes', 'dislikes', 'comment_count'])\n#df_test.show(1)\n","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687398_-121910152","id":"20191218-163624_2006519104","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103377"},{"text":"%spark.pyspark\n# Explore the correlation between target variables 'views' and independent variables\n# we will only use the following variables : category_id, likes, dislikes, comment_count\n\n#import six\n#for i in df_youtube.columns:\n#    if not( isinstance(df_youtube.select(i).take(1)[0][0], six.string_types)):\n#        print( \"Correlation to views for \", i, df_youtube.stat.corr('views',i))\n\n#print( \"Correlation to views for trending_date \", df_youtube.stat.corr('views','trending_date'))\nprint( \"Correlation to views for category_id \", df_youtube.stat.corr('views','category_id'))\nprint( \"Correlation to views for likes \", df_youtube.stat.corr('views','likes'))\nprint( \"Correlation to views for dislikes \", df_youtube.stat.corr('views','dislikes'))\nprint( \"Correlation to views for comment_count \", df_youtube.stat.corr('views','comment_count'))\n","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687398_-2048740499","id":"20191218-171023_538388805","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103378"},{"text":"%spark.pyspark\nfrom pyspark.ml.feature import VectorAssembler\n\n# Prepare data for linear regression\n# For the prediction, we will only use the following variables : category_id, likes, dislikes, comment_count\n\nvectorAssembler = VectorAssembler(inputCols = ['category_id', 'likes', 'dislikes', 'comment_count'], outputCol = 'features')\nvdf_youtube = vectorAssembler.transform(df_youtube)\n\n#vdf_youtube.show(1000)\n\n# The linear regression only needs 2 columns : 'features', whics is the values of all the independent variables and the target var 'views' \nvdf_youtube = vdf_youtube.select(['features', 'views'])\n\n#vdf_youtube.show(1000)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687399_-1886270793","id":"20191218-164709_2039962235","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103379"},{"text":"%spark.pyspark\ntype(vdf_youtube)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687399_-819782747","id":"20191218-185457_2026308192","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103380"},{"text":"%spark.pyspark\n\n# Split the data into training and testing set. We use 70% for training and 30% testing \n\ntrain_df, test_df = vdf_youtube.randomSplit([0.7, 0.3])\n\n#test_df.show(100)\n#train_df.show(30)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687400_-1615554704","id":"20191218-165633_12432031","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103381"},{"text":"%spark.pyspark\nfrom pyspark.ml.regression import LinearRegression\n\n# Train the model\n\nlr = LinearRegression(featuresCol = 'features', labelCol='views', maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlr_model = lr.fit(train_df)\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"Intercept: \" + str(lr_model.intercept))","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687400_1442496609","id":"20191218-171414_1158140316","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103382"},{"text":"%spark.pyspark\ntype(lr_model)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687401_734371819","id":"20191218-200148_682449979","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103383"},{"text":"%spark.pyspark\n\n# show the model summary \n\ntrainingSummary = lr_model.summary\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary.r2)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687401_-989440512","id":"20191218-171452_1491243240","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103384"},{"text":"%spark.pyspark\n\n#\n\ntrain_df.describe().show()","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687402_-770118919","id":"20191218-171513_991549184","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103385"},{"text":"%spark.pyspark\n\n\n\nlr_predictions = lr_model.transform(test_df)\nlr_predictions.select(\"prediction\",\"views\",\"features\").show(5)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"views\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687403_-1087813377","id":"20191218-171528_1996000808","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103386"},{"text":"%spark.pyspark\n\n# Predict on test data\n\ntest_result = lr_model.evaluate(test_df)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687413_1914904185","id":"20191218-171558_1233831171","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103387"},{"text":"%spark.pyspark\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\ntrainingSummary.residuals.show()","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687414_-839846472","id":"20191218-171639_421626110","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103388"},{"text":"%spark.pyspark\npredictions = lr_model.transform(test_df)\npredictions.select(\"prediction\",\"views\",\"features\").show()","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687415_-461869510","id":"20191218-171657_851520276","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103389"},{"text":"%spark.pyspark\n# Yang di bawah ini dijalanin lagi dengan data yang dinormalisasi\n# .....","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687415_-2100574734","id":"20191218-211743_1854943139","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103390"},{"text":"%spark.pyspark\n\nfrom pyspark.sql.functions import mean, stddev\n\ndef standardize_data(orig_df, columns):\n    '''\n    Add normalised columns to the input dataframe.\n    formula = [(X - mean) / std_dev]\n    Inputs : training dataframe, list of column name strings to be normalised\n    Returns : dataframe with new normalised columns, averages and std deviation dataframes \n    '''\n    # Find the Mean and the Standard Deviation for each column\n    aggExpr = []\n    aggStd = []\n    for column in columns:\n        aggExpr.append(mean(orig_df[column]).alias(column))\n        aggStd.append(stddev(orig_df[column]).alias(column + '_stddev'))\n    \n    averages = orig_df.agg(*aggExpr).collect()[0]\n    std_devs = orig_df.agg(*aggStd).collect()[0]\n    \n    # Standardise each dataframe, column by column\n    for column in columns:            \n        # Standardise the TRAINING data\n        orig_df = orig_df.withColumn(column + '_norm', ((orig_df[column] - averages[column]) / \n                                                              std_devs[column + '_stddev']))       \n    \n    return orig_df #, averages, std_devs","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687416_498934363","id":"20191218-171714_1086614595","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103391"},{"text":"%spark.pyspark\n\ndf_youtube_norm = standardize_data(df_youtube, ['category_id', 'likes', 'dislikes', 'comment_count', 'views'])\n\n#df_youtube_norm.show(5)","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687416_1736538944","id":"20191218-205259_1801622088","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103392"},{"text":"%spark.pyspark\nfrom pyspark.ml.feature import VectorAssembler\n\n#prepare data\nvectorAssembler = VectorAssembler(inputCols = ['category_id_norm', 'likes_norm', 'dislikes_norm', 'comment_count_norm'], outputCol = 'features')\nvdf_youtube_norm = vectorAssembler.transform(df_youtube_norm)\nvdf_youtube_norm = vdf_youtube_norm.select(['features', 'views_norm','views'])\n\nvdf_youtube_norm.show(10)\n\n#split train-test\ntrain_df_norm, test_df_norm = vdf_youtube_norm.randomSplit([0.7, 0.3])\n\n","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687417_-604984560","id":"20191218-205458_1555357623","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103393"},{"text":"%spark.pyspark\n\nfrom pyspark.ml.regression import LinearRegression\n\ntarget_var = 'views'\n\n# Train the model\n\nlr_norm = LinearRegression(featuresCol = 'features', labelCol=target_var, maxIter=10, regParam=0.3, elasticNetParam=0.8)\nlr_model_norm = lr_norm.fit(train_df_norm)\nprint(\"Coefficients: \" + str(lr_model_norm.coefficients))\nprint(\"Intercept: \" + str(lr_model_norm.intercept))\n\n# show the model summary \n\ntrainingSummary_norm = lr_model_norm.summary\nprint(\"RMSE: %f\" % trainingSummary_norm.rootMeanSquaredError)\nprint(\"r2: %f\" % trainingSummary_norm.r2)\n\ntrain_df_norm.describe().show()\n\n\n#evaluate\n\nlr_predictions_norm = lr_model_norm.transform(test_df_norm)\nlr_predictions_norm.select(\"prediction\",target_var,\"features\").show(15)\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nlr_evaluator_norm = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"views\",metricName=\"r2\")\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator_norm.evaluate(lr_predictions_norm))\n\n# Predict on test data\n\ntest_result_norm = lr_model_norm.evaluate(test_df_norm)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result_norm.rootMeanSquaredError)\n\nprint(\"numIterations: %d\" % trainingSummary_norm.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary_norm.objectiveHistory))\ntrainingSummary_norm.residuals.show()\n\npredictions_norm = lr_model_norm.transform(test_df_norm)\npredictions_norm.select(\"prediction\",target_var,\"features\").show()\n","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687417_-925814270","id":"20191218-210020_1274323484","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103394"},{"text":"%spark.pyspark\n","user":"yava","dateUpdated":"2019-12-18T20:41:27-0500","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576719687418_542371648","id":"20191218-210641_1887523906","dateCreated":"2019-12-18T20:41:27-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103395"}],"name":"Big Data Analytics Quick Start/LABS 08: Machine Learning With Spark","id":"2EX9YP2FE","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"hive:yava:":[],"sh:yava:":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}